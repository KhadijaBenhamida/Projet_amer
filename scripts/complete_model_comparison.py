"""
Script de comparaison compl√®te de tous les mod√®les

Compare les performances de :
- Persistence Model
- Seasonal Naive
- Linear Regression
- XGBoost (si disponible)
- LSTM (si disponible)

G√©n√®re :
- Tableau comparatif avec toutes les m√©triques (RMSE, MAE, R¬≤, MAPE)
- Visualisations (bar charts, radar chart, time series)
- Rapport d√©taill√© en Markdown

Author: Climate Prediction Team
Date: December 2025
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import logging
import json
from typing import Dict, List

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Style pour les graphiques
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (15, 10)
plt.rcParams['font.size'] = 10


class ModelComparator:
    """
    Classe pour comparer les performances de tous les mod√®les.
    """
    
    def __init__(self, base_path: Path):
        self.base_path = base_path
        self.results = {}
        self.models_info = {}
        
    def load_baseline_results(self):
        """Charge les r√©sultats des baselines."""
        logger.info("üìÇ Chargement des r√©sultats baselines...")
        
        baseline_csv = self.base_path / 'models' / 'baselines' / 'baseline_comparison.csv'
        if baseline_csv.exists():
            df = pd.read_csv(baseline_csv)
            for _, row in df.iterrows():
                model_name = row['Model']
                self.results[model_name] = {
                    'MSE': row['MSE'],
                    'RMSE': row['RMSE'],
                    'MAE': row['MAE'],
                    'R2': row['R2'],
                    'MAPE': 0.0  # Non calcul√© pour baselines
                }
            logger.info(f"   ‚úÖ {len(df)} baselines charg√©s")
        else:
            logger.warning("   ‚ö†Ô∏è  Baseline results non trouv√©s")
    
    def load_lstm_results(self):
        """Charge les r√©sultats LSTM."""
        logger.info("üìÇ Chargement des r√©sultats LSTM...")
        
        lstm_metrics = self.base_path / 'models' / 'lstm' / 'lstm_metrics.csv'
        if lstm_metrics.exists():
            df = pd.read_csv(lstm_metrics)
            self.results['LSTM'] = {
                'MSE': df['MSE'].values[0],
                'RMSE': df['RMSE'].values[0],
                'MAE': df['MAE'].values[0],
                'R2': df['R2'].values[0],
                'MAPE': df['MAPE'].values[0]
            }
            logger.info("   ‚úÖ LSTM charg√©")
        else:
            logger.warning("   ‚ö†Ô∏è  LSTM results non trouv√©s")
    
    def load_xgboost_results(self):
        """Charge les r√©sultats XGBoost."""
        logger.info("üìÇ Chargement des r√©sultats XGBoost...")
        
        xgb_metrics = self.base_path / 'models' / 'xgboost' / 'xgboost_metrics.csv'
        if xgb_metrics.exists():
            df = pd.read_csv(xgb_metrics)
            self.results['XGBoost'] = {
                'MSE': df['MSE'].values[0],
                'RMSE': df['RMSE'].values[0],
                'MAE': df['MAE'].values[0],
                'R2': df['R2'].values[0],
                'MAPE': df['MAPE'].values[0]
            }
            logger.info("   ‚úÖ XGBoost charg√©")
        else:
            logger.warning("   ‚ö†Ô∏è  XGBoost results non trouv√©s")
    
    def create_comparison_table(self) -> pd.DataFrame:
        """Cr√©e le tableau de comparaison."""
        logger.info("üìä Cr√©ation du tableau comparatif...")
        
        df = pd.DataFrame(self.results).T
        df = df.sort_values('RMSE')
        
        logger.info(f"   ‚úÖ {len(df)} mod√®les compar√©s")
        return df
    
    def plot_rmse_comparison(self, df: pd.DataFrame, save_path: Path):
        """Bar chart de comparaison des RMSE."""
        fig, ax = plt.subplots(figsize=(12, 6))
        
        colors = ['#2ecc71' if i == df['RMSE'].idxmin() else '#3498db' for i in df.index]
        df['RMSE'].plot(kind='bar', ax=ax, color=colors, alpha=0.8)
        
        ax.set_ylabel('RMSE (¬∞C)', fontsize=12)
        ax.set_xlabel('Mod√®le', fontsize=12)
        ax.set_title('Comparaison RMSE - Tous les mod√®les', fontsize=14, fontweight='bold')
        ax.grid(axis='y', alpha=0.3)
        
        # Ajouter les valeurs sur les barres
        for i, (idx, val) in enumerate(df['RMSE'].items()):
            ax.text(i, val + 0.5, f'{val:.3f}¬∞C', ha='center', fontsize=10, fontweight='bold')
        
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logger.info(f"   ‚úÖ RMSE chart sauvegard√©: {save_path}")
        plt.close()
    
    def plot_all_metrics_comparison(self, df: pd.DataFrame, save_path: Path):
        """Bar chart group√© pour toutes les m√©triques."""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        metrics = ['RMSE', 'MAE', 'R2', 'MAPE']
        titles = ['RMSE (¬∞C)', 'MAE (¬∞C)', 'R¬≤ Score', 'MAPE (%)']
        
        for ax, metric, title in zip(axes.flat, metrics, titles):
            colors = ['#2ecc71' if i == df[metric].idxmin() else '#3498db' for i in df.index]
            if metric == 'R2':  # Pour R¬≤, on veut le max
                colors = ['#2ecc71' if i == df[metric].idxmax() else '#3498db' for i in df.index]
            
            df[metric].plot(kind='bar', ax=ax, color=colors, alpha=0.8)
            ax.set_title(title, fontsize=12, fontweight='bold')
            ax.set_ylabel('Valeur', fontsize=10)
            ax.grid(axis='y', alpha=0.3)
            ax.tick_params(axis='x', rotation=45)
            
            # Ajouter les valeurs
            for i, (idx, val) in enumerate(df[metric].items()):
                ax.text(i, val, f'{val:.3f}', ha='center', va='bottom', fontsize=8)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logger.info(f"   ‚úÖ All metrics chart sauvegard√©: {save_path}")
        plt.close()
    
    def plot_radar_chart(self, df: pd.DataFrame, save_path: Path):
        """Radar chart pour comparaison multi-dimensionnelle."""
        # Normaliser les m√©triques pour le radar chart (0-1)
        df_norm = df.copy()
        df_norm['RMSE_norm'] = 1 - (df_norm['RMSE'] / df_norm['RMSE'].max())
        df_norm['MAE_norm'] = 1 - (df_norm['MAE'] / df_norm['MAE'].max())
        df_norm['R2_norm'] = df_norm['R2']
        df_norm['MAPE_norm'] = 1 - (df_norm['MAPE'] / df_norm['MAPE'].max())
        
        # S√©lectionner top 3 mod√®les
        top_models = df.nsmallest(3, 'RMSE').index
        
        categories = ['RMSE\n(invers√©)', 'MAE\n(invers√©)', 'R¬≤', 'MAPE\n(invers√©)']
        N = len(categories)
        
        angles = [n / float(N) * 2 * np.pi for n in range(N)]
        angles += angles[:1]
        
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
        colors = ['#e74c3c', '#3498db', '#2ecc71']
        for model, color in zip(top_models, colors):
            values = [
                df_norm.loc[model, 'RMSE_norm'],
                df_norm.loc[model, 'MAE_norm'],
                df_norm.loc[model, 'R2_norm'],
                df_norm.loc[model, 'MAPE_norm']
            ]
            values += values[:1]
            
            ax.plot(angles, values, 'o-', linewidth=2, label=model, color=color)
            ax.fill(angles, values, alpha=0.25, color=color)
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, fontsize=11)
        ax.set_ylim(0, 1)
        ax.set_title('Comparaison Multi-M√©trique (Top 3 Mod√®les)', 
                     fontsize=14, fontweight='bold', y=1.08)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)
        ax.grid(True)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logger.info(f"   ‚úÖ Radar chart sauvegard√©: {save_path}")
        plt.close()
    
    def generate_markdown_report(self, df: pd.DataFrame, save_path: Path):
        """G√©n√®re un rapport Markdown d√©taill√©."""
        logger.info("üìÑ G√©n√©ration du rapport Markdown...")
        
        report = f"""# üìä Rapport de Comparaison des Mod√®les
## Projet: Pr√©diction de Temp√©rature Climatique

Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## üèÜ R√©sultats Globaux

### Tableau Comparatif Complet

| Mod√®le | RMSE (¬∞C) | MAE (¬∞C) | R¬≤ | MAPE (%) | Rang |
|--------|-----------|----------|-----|----------|------|
"""
        
        for rank, (model, row) in enumerate(df.iterrows(), 1):
            report += f"| **{model}** | {row['RMSE']:.4f} | {row['MAE']:.4f} | {row['R2']:.4f} | {row['MAPE']:.2f} | {rank} |\n"
        
        best_model = df['RMSE'].idxmin()
        best_rmse = df.loc[best_model, 'RMSE']
        
        report += f"""
---

## ü•á Meilleur Mod√®le: **{best_model}**

### Performances:
- **RMSE**: {best_rmse:.4f}¬∞C
- **MAE**: {df.loc[best_model, 'MAE']:.4f}¬∞C
- **R¬≤**: {df.loc[best_model, 'R2']:.4f}
- **MAPE**: {df.loc[best_model, 'MAPE']:.2f}%

### Interpr√©tation:
- Le mod√®le **{best_model}** atteint une pr√©cision de **¬±{best_rmse:.2f}¬∞C**
- Il explique **{df.loc[best_model, 'R2']*100:.2f}%** de la variance
- Erreur moyenne absolue de **{df.loc[best_model, 'MAE']:.2f}¬∞C**

---

## üìà Analyse Comparative

### Baselines vs Machine Learning

"""
        
        # Comparer baselines vs ML
        baselines = [m for m in df.index if 'Persistence' in m or 'Seasonal' in m or 'Linear Regression' in m]
        ml_models = [m for m in df.index if m not in baselines]
        
        if ml_models:
            best_baseline = df.loc[baselines, 'RMSE'].min()
            best_ml = df.loc[ml_models, 'RMSE'].min()
            improvement = ((best_baseline - best_ml) / best_baseline) * 100
            
            report += f"""
**Am√©lioration ML vs Baselines**: {improvement:.2f}%
- Meilleur baseline: {best_baseline:.4f}¬∞C
- Meilleur ML: {best_ml:.4f}¬∞C
- Gain de pr√©cision: {best_baseline - best_ml:.4f}¬∞C

"""
        
        report += f"""
---

## üéØ Recommandations

### Pour la Production:
1. **Mod√®le recommand√©**: {best_model}
2. **Pr√©cision attendue**: ¬±{best_rmse:.2f}¬∞C
3. **Cas d'usage**: Pr√©diction temp√©rature climatique en temps r√©el

### Pour l'Am√©lioration:
- Feature Engineering suppl√©mentaire (interactions, polynomial features)
- Ensemble methods (stacking, voting)
- Hyperparameter tuning avanc√©
- Donn√©es m√©t√©o suppl√©mentaires (satellite, radar)

---

## üìÅ Fichiers G√©n√©r√©s

- `model_comparison_rmse.png`: Comparaison RMSE
- `model_comparison_all_metrics.png`: Toutes les m√©triques
- `model_comparison_radar.png`: Radar chart
- `model_comparison_results.csv`: Donn√©es compl√®tes
- `model_comparison_report.md`: Ce rapport

---

*Rapport g√©n√©r√© automatiquement par complete_model_comparison.py*
"""
        
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"   ‚úÖ Rapport sauvegard√©: {save_path}")
    
    def run_comparison(self):
        """Ex√©cute la comparaison compl√®te."""
        logger.info("=" * 80)
        logger.info("üìä COMPARAISON COMPL√àTE DES MOD√àLES")
        logger.info("=" * 80)
        
        # Charger tous les r√©sultats
        self.load_baseline_results()
        self.load_lstm_results()
        self.load_xgboost_results()
        
        if not self.results:
            logger.error("‚ùå Aucun r√©sultat trouv√©!")
            return
        
        # Cr√©er tableau
        df = self.create_comparison_table()
        
        # Cr√©er dossier de sortie
        output_dir = self.base_path / 'results' / 'model_comparison'
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Sauvegarder CSV
        csv_path = output_dir / 'model_comparison_results.csv'
        df.to_csv(csv_path)
        logger.info(f"‚úÖ R√©sultats sauvegard√©s: {csv_path}")
        
        # Afficher r√©sultats
        logger.info("\n" + "=" * 80)
        logger.info("üìä R√âSULTATS DE COMPARAISON")
        logger.info("=" * 80)
        print(df.to_string())
        
        # G√©n√©rer visualisations
        logger.info("\n" + "=" * 80)
        logger.info("üìä G√âN√âRATION DES VISUALISATIONS")
        logger.info("=" * 80)
        
        self.plot_rmse_comparison(df, output_dir / 'model_comparison_rmse.png')
        self.plot_all_metrics_comparison(df, output_dir / 'model_comparison_all_metrics.png')
        self.plot_radar_chart(df, output_dir / 'model_comparison_radar.png')
        
        # G√©n√©rer rapport
        self.generate_markdown_report(df, output_dir / 'model_comparison_report.md')
        
        logger.info("\n" + "=" * 80)
        logger.info("‚úÖ COMPARAISON TERMIN√âE")
        logger.info("=" * 80)
        logger.info(f"üìÅ R√©sultats dans: {output_dir}")
        
        # Meilleur mod√®le
        best_model = df['RMSE'].idxmin()
        best_rmse = df.loc[best_model, 'RMSE']
        logger.info(f"\nüèÜ MEILLEUR MOD√àLE: {best_model}")
        logger.info(f"   RMSE: {best_rmse:.4f}¬∞C")
        logger.info(f"   R¬≤: {df.loc[best_model, 'R2']:.4f}")


def main():
    """Fonction principale."""
    base_path = Path(__file__).parent.parent
    
    comparator = ModelComparator(base_path)
    comparator.run_comparison()


if __name__ == "__main__":
    main()
